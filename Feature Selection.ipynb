{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "martial-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fresh-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = pd.read_csv(\"BankChurnersNormalized.csv\")\n",
    "data_old = pd.read_csv(\"BankChurners.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "international-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ME(Y_test, predicted):\n",
    "    print (\"Accuracy Score :\",accuracy_score(Y_test, predicted))\n",
    "    print (\"Confusion Matrix :\") \n",
    "    print(confusion_matrix(Y_test, predicted))  \n",
    "    print (\"Report : \") \n",
    "    print (classification_report(Y_test, predicted))\n",
    "    \n",
    "def Random_Forest(X_train, X_test, Y_train):\n",
    "    classifier =  RandomForestClassifier()\n",
    "    classifier = classifier.fit(X_train, Y_train)\n",
    "    predicted = classifier.predict(X_test)\n",
    "    return predicted    \n",
    "\n",
    "def feat_sel(X, Y):\n",
    "    feats_sel = SelectKBest(chi2)\n",
    "    fit = feats_sel.fit(X, Y)\n",
    "    scores = pd.DataFrame(fit.scores_)\n",
    "    columns = pd.DataFrame(data_old.columns[2:21])\n",
    "    feats = pd.concat([columns, scores], axis=1)\n",
    "    feats.columns = ('Features', 'Score')\n",
    "    useful_feats = []\n",
    "    useless_feats = []\n",
    "    for feat in feats.values:\n",
    "        if feat[1] > 10:\n",
    "            useful_feats.append(feat[0])\n",
    "        if feat[1] < 0.1:\n",
    "            useless_feats.append(feat[0])\n",
    "    return (useful_feats, useless_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fitted-bandwidth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before feature selection:\n",
      "\n",
      "Accuracy Score : 0.9504455553140622\n",
      "Confusion Matrix :\n",
      "[[3799   52]\n",
      " [ 176  574]]\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.99      0.97      3851\n",
      "         1.0       0.92      0.77      0.83       750\n",
      "\n",
      "    accuracy                           0.95      4601\n",
      "   macro avg       0.94      0.88      0.90      4601\n",
      "weighted avg       0.95      0.95      0.95      4601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Before feature selection:\\n\")\n",
    "X_b = data_new[data_new.columns[1:20]]\n",
    "Y = data_new[data_new.columns[0]]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_b, Y, test_size=0.5)\n",
    "pred_before = Random_Forest(X_train, X_test, Y_train)\n",
    "show_ME(Y_test, pred_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "floppy-munich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After feature selection:\n",
      "\n",
      "Accuracy Score : 0.9556618126494241\n",
      "Confusion Matrix :\n",
      "[[3813   76]\n",
      " [ 128  584]]\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.98      0.97      3889\n",
      "         1.0       0.88      0.82      0.85       712\n",
      "\n",
      "    accuracy                           0.96      4601\n",
      "   macro avg       0.93      0.90      0.91      4601\n",
      "weighted avg       0.95      0.96      0.95      4601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"After feature selection:\\n\")\n",
    "X_a = data_new[feat_sel(X_b, Y)[0]]\n",
    "Y = data_new[data_new.columns[0]]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_a, Y, test_size=0.5)\n",
    "pred_after = Random_Forest(X_train, X_test, Y_train)\n",
    "show_ME(Y_test, pred_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "annual-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useful features:  ['Total_Relationship_Count', 'Months_Inactive_12_mon', 'Contacts_Count_12_mon', 'Total_Revolving_Bal', 'Total_Trans_Amt', 'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']\n",
      "Useless features:  ['Marital_Status', 'Card_Category', 'Months_on_book', 'Avg_Open_To_Buy']\n"
     ]
    }
   ],
   "source": [
    "useful_f, useless_f = feat_sel(X_b, Y)\n",
    "print(\"Useful features: \", useful_f)\n",
    "print(\"Useless features: \", useless_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
